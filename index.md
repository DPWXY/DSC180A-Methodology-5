# Website for MA 5
**Name**: Xiaoyue Wang

**Email**: xiw027@ucsd.edu

**Section**: B17

**Mentor**: Hao Zhang

## Answers for prompts
### 1. What is the most interesting topic covered in your domain this quarter?
The most intriguing topic in my domain this quarter delves into the architecture and mechanics of transformers, focusing on calculations of floating-point operations per second (FLOPS) and memory usage. As the realm of large language models (LLMs) expands, gaining a foundational understanding and hands-on experience in training these models is crucial. This knowledge not only deepens our comprehension of LLMs but also enhances our ability to optimize and innovate in this rapidly evolving field.

### 2. Describe a potential investigation you would like to pursue for your Quarter 2 Project.
For my Quarter 2 project, I am considering exploring the implementation of a content window extension in our models. Presently, our models are limited to processing only short context lengths. By expanding this window, we can enable the models to process and predict longer text inputs. This enhancement could significantly improve the models' understanding and generation of lengthy and complex textual data, opening up new possibilities in natural language processing.

### 3. What is a potential change youâ€™d make to the approach taken in your current Quarter 1 Project?
In my current Quarter 1 Project, a key modification I'm contemplating is the optimization of our training pipelines. This would involve refining the processes for data loading and tokenization, aligning them more efficiently with the model training phase. Such an adjustment aims to streamline the overall training process, potentially leading to faster model development and more effective learning outcomes.

### 4. What other techniques would you be interested in using in your project?
In my project, I'm particularly interested in experimenting with data parallel techniques. These techniques are instrumental in enhancing the efficiency of model training, especially when dealing with large datasets and complex models. By distributing the data and computation across multiple processors or machines, data parallelism can significantly accelerate the training process and improve scalability, making it a valuable tool in the realm of machine learning and AI. 
